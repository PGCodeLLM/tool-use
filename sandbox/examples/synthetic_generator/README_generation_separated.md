# Generation Separated - Optimized Model Evaluation Pipeline

A performance-optimized version of the model evaluation pipeline that separates inference and execution phases for better resource utilization and faster iteration.

## Overview

The original `generation_custom.py` creates a new sandbox for each task, which creates significant overhead. This separated version splits the evaluation into two independent phases:

1. **Phase 1 - Batch Inference**: Generate all commands upfront using the model
2. **Phase 2 - Batch Execution**: Execute pre-generated commands through sandboxes

## Performance Benefits

- **ðŸš€ 50-80% faster** - No sandbox overhead during inference
- **ðŸ”„ Easy retry** - Retry failed executions without re-inference  
- **ðŸ“Š Better monitoring** - Separate progress tracking for each phase
- **âš¡ Independent scaling** - Run inference on fast machines, execution on sandbox-optimized machines

## Architecture

```
Dataset â†’ Phase 1: Inference â†’ commands.jsonl â†’ Phase 2: Execution â†’ results.jsonl
                â†“                                        â†“
         Model API calls                        Sandbox operations
        (fast, parallel)                      (slower, sequential)
```

## Installation

The script uses the same dependencies as the original:

```bash
# Install dependencies (using uv)
uv run generation_separated.py --help

# Or install manually
pip install openai>=1.6.0 datasets>=3.6.0 tenacity>=8.0.0 httpx>=0.27.0 rich>=13.9.5
```

## Usage

### Run Both Phases (Default)

```bash
# Complete evaluation pipeline
python generation_separated.py \
    --model gpt-4 \
    --dataset path/to/dataset.jsonl \
    --base-url https://api.openai.com/v1 \
    --api-key your-api-key

# With custom parameters
python generation_separated.py \
    --model gpt-3.5-turbo \
    --dataset deathbyknowledge/shell-tasks \
    --max-samples 100 \
    --temperature 0.7 \
    --output-dir my_results
```

### Phase 1 Only: Inference

Generate commands without running sandboxes (useful for quick iteration on prompts):

```bash
python generation_separated.py \
    --phase inference \
    --model gpt-4 \
    --dataset path/to/dataset.jsonl \
    --output-dir results
```

This creates `results/commands.jsonl` with pre-generated commands.

### Phase 2 Only: Execution  

Execute pre-generated commands through sandboxes:

```bash
python generation_separated.py \
    --phase execution \
    --commands-file results/commands.jsonl \
    --output-dir results \
    --sos-port 3000
```

This reads from `commands.jsonl` and creates `results/results.jsonl`.

### Mock Mode for Testing

Test the pipeline without API calls:

```bash
python generation_separated.py \
    --model mock \
    --dataset test_data.jsonl \
    --max-samples 10
```

## File Formats

### Input Dataset (JSONL)

Each line should contain:
```json
{
    "task": "Create a file named 'hello.txt' with content 'Hello World'",
    "setup_commands": ["mkdir -p /tmp/test", "cd /tmp/test"],
    "success_condition": "test -f hello.txt && grep -q 'Hello World' hello.txt"
}
```

### Commands File (Intermediate)

Generated by Phase 1:
```json
{
    "task_id": "a1b2c3d4",
    "task": "Create a file named 'hello.txt' with content 'Hello World'",  
    "setup_commands": ["mkdir -p /tmp/test", "cd /tmp/test"],
    "success_condition": "test -f hello.txt && grep -q 'Hello World' hello.txt",
    "generated_command": "echo 'Hello World' > hello.txt",
    "timestamp": "2024-01-01T12:00:00Z"
}
```

### Results File (Final Output)

Generated by Phase 2:
```json
{
    "task_id": "a1b2c3d4",
    "task": "Create a file named 'hello.txt' with content 'Hello World'",
    "setup_commands": ["mkdir -p /tmp/test", "cd /tmp/test"], 
    "success_condition": "test -f hello.txt && grep -q 'Hello World' hello.txt",
    "generated_command": "echo 'Hello World' > hello.txt",
    "command_output": "",
    "command_exit_code": 0,
    "command_success": true,
    "test_exit_code": 0,
    "test_passed": true,
    "overall_success": true,
    "trajectory": [...],
    "start_time": "2024-01-01T12:00:00Z",
    "end_time": "2024-01-01T12:00:05Z"
}
```

## Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `--model` | Required | Model name (e.g., 'gpt-4', 'mock') |
| `--dataset` | Required | Dataset path or HuggingFace name |
| `--phase` | `both` | Phase to run: `inference`, `execution`, or `both` |
| `--output-dir` | `evaluation_results` | Output directory for all files |
| `--max-samples` | None | Limit number of samples (useful for testing) |
| `--temperature` | 0.6 | Model sampling temperature |
| `--max-tokens` | 2000 | Maximum tokens to generate |
| `--base-url` | OpenAI default | API base URL for model |
| `--api-key` | `OPENAI_API_KEY` env | API key for model service |
| `--sos-port` | 3000 | Port for SoS sandbox server |
| `--sandbox-image` | `deathbyknowledge/shellm-sandbox:latest` | Docker image |
| `--commands-file` | None | Commands file (required for execution-only) |

## Environment Variables

```bash
export OPENAI_API_KEY="your-api-key"
export OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional
```

## Output Files

The script generates several output files in `--output-dir`:

- `commands.jsonl` - Pre-generated commands (Phase 1 output)
- `results.jsonl` - Execution results (Phase 2 output) 
- `summary.json` - Evaluation summary with success rates

## Error Handling

- **Command generation failures**: Saved as error records in `commands.jsonl`
- **Sandbox execution failures**: Saved as failed results in `results.jsonl`
- **Retries**: Built-in retry logic for API calls and sandbox operations
- **Graceful degradation**: Continues evaluation even if individual tasks fail

## Performance Tips

### For Maximum Speed
1. **Separate phases**: Run inference on GPU machines, execution on sandbox servers
2. **Parallel inference**: The script processes inference sequentially - consider running multiple instances with different dataset chunks
3. **Mock mode**: Use `--model mock` for rapid pipeline testing
4. **Limit samples**: Use `--max-samples` during development

### For Production
1. **Monitor resources**: Check SoS server capacity
2. **Batch processing**: Process large datasets in chunks
3. **Resume capability**: Phase separation allows easy resumption from failures

## Troubleshooting

### Common Issues

**"No commands found in commands file"**
- Run inference phase first or check file path

**"API key is required"**  
- Set `OPENAI_API_KEY` environment variable or use `--api-key`

**"Connection refused"**
- Ensure SoS server is running on specified port (`--sos-port`)

**"Sandbox creation failed"**
- Check Docker is running and sandbox image is available

### Debug Mode

Add verbose logging by modifying the script to enable debug output:
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## Comparison with Original

| Feature | Original (`generation_custom.py`) | Separated (`generation_separated.py`) |
|---------|-----------------------------------|--------------------------------------|
| **Performance** | Sequential sandbox creation | Batch inference + execution |
| **Retry Logic** | Full re-run required | Retry phases independently |  
| **Progress Tracking** | Single progress bar | Separate progress per phase |
| **Resource Usage** | Mixed API + sandbox load | Optimized for each phase |
| **Flexibility** | All-or-nothing execution | Phase-specific control |
| **File Output** | Single results file | Intermediate + final files |

## Future Optimizations

This separated architecture enables further optimizations:

1. **Concurrent execution**: Parallelize Phase 2 sandbox operations
2. **Sandbox pooling**: Reuse sandboxes across tasks  
3. **Smart batching**: Group tasks by setup requirements
4. **Distributed execution**: Run phases on different machines
5. **Incremental processing**: Resume from partial completions

## Contributing

When making changes:
1. Maintain backward compatibility with existing datasets
2. Preserve the two-phase separation principle  
3. Add appropriate error handling and logging
4. Update this documentation for new features

## License

Same license as the original project.